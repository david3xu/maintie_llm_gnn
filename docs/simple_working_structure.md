# MaintIE LLM-GNN: Simple Working Directory Structure

**Code Priority**: Working end-to-end pipeline first  
**Start Simple**: Minimal but complete implementation  
**Professional**: Clean architecture with proper separation  
**Good Lifecycle**: Development â†’ Testing â†’ Production workflow

---

## ğŸ“ **Complete Working Structure (Week 1)**

```
maintie_llm_gnn/                               # Root project
â”‚
â”œâ”€â”€ ğŸ“Š data/                                   # Data storage
â”‚   â”œâ”€â”€ raw/                                   # Original MaintIE files
â”‚   â”‚   â”œâ”€â”€ gold_release.json                 # ~1000 expert samples
â”‚   â”‚   â”œâ”€â”€ silver_release.json               # ~7000 auto samples  
â”‚   â”‚   â””â”€â”€ scheme.json                       # Entity/relation ontology
â”‚   â”œâ”€â”€ processed/                             # Generated artifacts
â”‚   â”‚   â”œâ”€â”€ embeddings.pkl                    # LLM embeddings
â”‚   â”‚   â”œâ”€â”€ graphs.pkl                        # Built graphs
â”‚   â”‚   â””â”€â”€ splits.pkl                        # Train/val splits
â”‚   â””â”€â”€ README.md                              # Data documentation
â”‚
â”œâ”€â”€ ğŸ§  src/                                    # Core implementation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing/                       # Data pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py                     # ğŸ”¥ Simple JSON loading
â”‚   â”‚   â”œâ”€â”€ embedding_generator.py             # ğŸ”¥ LLM embeddings  
â”‚   â”‚   â””â”€â”€ graph_builder.py                  # ğŸ”¥ Basic graph construction
â”‚   â”œâ”€â”€ models/                                # Model architecture
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_gnn_hybrid.py                 # ğŸ”¥ Main hybrid model
â”‚   â”‚   â””â”€â”€ simple_gnn.py                     # ğŸ”¥ Basic GNN implementation
â”‚   â”œâ”€â”€ training/                              # Training system
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ simple_trainer.py                 # ğŸ”¥ Basic training loop
â”‚   â”‚   â””â”€â”€ utils.py                          # Training utilities
â”‚   â””â”€â”€ evaluation/                            # Evaluation system
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ evaluator.py                      # ğŸ”¥ Basic evaluation
â”‚       â””â”€â”€ metrics.py                        # Performance metrics
â”‚
â”œâ”€â”€ âš™ï¸ config/                                 # Configuration
â”‚   â”œâ”€â”€ config.yaml                           # ğŸ”¥ Main configuration
â”‚   â””â”€â”€ complexity_levels.yaml                # Entity complexity mappings
â”‚
â”œâ”€â”€ ğŸ§ª tests/                                 # Testing framework
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_loading.py                  # Data pipeline tests
â”‚   â”œâ”€â”€ test_model.py                         # Model tests
â”‚   â””â”€â”€ test_training.py                      # Training tests
â”‚
â”œâ”€â”€ ğŸ“Š results/                               # Experiment results
â”‚   â”œâ”€â”€ training/                             # Training outputs
â”‚   â”œâ”€â”€ evaluation/                           # Evaluation results
â”‚   â””â”€â”€ models/                               # Saved model checkpoints
â”‚
â”œâ”€â”€ ğŸš€ scripts/                               # Automation scripts
â”‚   â”œâ”€â”€ train.py                              # ğŸ”¥ Training script
â”‚   â”œâ”€â”€ evaluate.py                           # ğŸ”¥ Evaluation script
â”‚   â””â”€â”€ setup.py                              # Environment setup
â”‚
â”œâ”€â”€ Makefile                                   # ğŸ”¥ Automation commands
â”œâ”€â”€ requirements.txt                           # Python dependencies
â”œâ”€â”€ .env.example                              # Environment template
â”œâ”€â”€ .gitignore                                # Git ignore rules
â””â”€â”€ README.md                                 # Project documentation
```

---

## ğŸ”¥ **Core Working Files (Minimum Viable Product)**

### **Essential Implementation Files:**
1. **`src/data_processing/data_loader.py`** - Simple JSON loading
2. **`src/data_processing/embedding_generator.py`** - Basic LLM embeddings
3. **`src/data_processing/graph_builder.py`** - Simple semantic similarity graphs
4. **`src/models/llm_gnn_hybrid.py`** - Working hybrid model
5. **`src/training/simple_trainer.py`** - Basic training loop
6. **`src/evaluation/evaluator.py`** - Simple F1 evaluation

### **Essential Configuration:**
7. **`config/config.yaml`** - Working configuration
8. **`Makefile`** - Automation commands

### **Essential Scripts:**
9. **`scripts/train.py`** - CLI training
10. **`scripts/evaluate.py`** - CLI evaluation

---

## ğŸš€ **Complete Working Workflow**

### **Step 1: Setup & Data Loading**
```bash
# Environment setup
make setup
make install-deps

# Data pipeline
make load-data          # src.data_processing.data_loader
make generate-embeddings # src.data_processing.embedding_generator  
make build-graphs       # src.data_processing.graph_builder
```

### **Step 2: Training**
```bash
# Simple training
make train              # scripts/train.py â†’ src.training.simple_trainer
```

### **Step 3: Evaluation**
```bash
# Basic evaluation
make evaluate           # scripts/evaluate.py â†’ src.evaluation.evaluator
```

### **Step 4: Testing**
```bash
# Validation
make test               # Run tests/ directory
```

---

## ğŸ“‹ **Makefile Commands (Complete Workflow)**

```makefile
# Setup commands
setup:          ## Initialize project environment
install-deps:   ## Install Python dependencies
load-data:      ## Load MaintIE data files

# Data processing commands  
generate-embeddings:  ## Create LLM embeddings
build-graphs:         ## Build semantic similarity graphs

# Training commands
train:          ## Run basic training
train-simple:   ## Alias for train

# Evaluation commands
evaluate:       ## Run basic evaluation
evaluate-gold:  ## Evaluate on gold standard

# Testing commands
test:           ## Run unit tests
test-pipeline:  ## Test end-to-end pipeline

# Status commands
status:         ## Show project status
clean:          ## Clean generated files
```

---

## ğŸ¯ **Professional Architecture Principles**

### **Code Priority** âœ…
- **Working first**: Each file has simple, functional implementation
- **No templates**: Real code that loads data, trains models, produces results
- **Testable**: Every core component has corresponding test

### **Start Simple** âœ…
- **Single-purpose files**: Each file does one thing well
- **Minimal dependencies**: Core workflow with essential libraries only
- **Basic algorithms**: Simple similarity, basic GNN, straightforward training

### **Professional** âœ…
- **Clean separation**: Data, models, training, evaluation clearly separated
- **Proper imports**: `__init__.py` files for clean module structure
- **Configuration management**: YAML configs, not hardcoded values
- **Documentation**: README files explaining each component

### **Good Lifecycle** âœ…
- **Development**: `src/` for implementation, `config/` for settings
- **Testing**: `tests/` directory with unit and integration tests
- **Production**: `scripts/` for CLI, `results/` for outputs
- **Automation**: `Makefile` for reproducible workflows

---

## âœ… **Success Criteria**

### **End-to-End Working Pipeline:**
```bash
# This complete workflow must work:
make setup
make load-data
make generate-embeddings  
make build-graphs
make train
make evaluate
# Output: Entity F1 score, Relation F1 score, trained model saved
```

### **File Count: ~20 files total**
- **10 core implementation files** (data, models, training, evaluation)
- **3 configuration files** (config, requirements, Makefile)
- **4 test files** (data, model, training, evaluation tests)
- **3 automation files** (train, evaluate, setup scripts)

### **Complexity: Minimal but complete**
- **No advanced features** (curriculum learning, caching, monitoring)
- **Basic algorithms** (cosine similarity, simple GNN, Adam optimizer)
- **Essential functionality** (load â†’ embed â†’ graph â†’ train â†’ evaluate)

**This structure provides the simplest possible implementation that works end-to-end while maintaining professional architecture for future enhancement.**